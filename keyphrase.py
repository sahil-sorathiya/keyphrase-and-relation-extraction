# -*- coding: utf-8 -*-
"""keyphrase

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/keyphrase-281bcd9c-7436-480c-9e7f-413971da61fa.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240410/auto/storage/goog4_request%26X-Goog-Date%3D20240410T162945Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dca2998df3218db25f5ea85a5b0c040415f71836f3d790899b075c3788b4abb15c46ccb705776ae921a4c6566161dda3d4b58d0532a9c418a536172068788462588620c5acc160d2c14e4fb51dedff92416e23477727386cf91afe245f8020fef313d1ff8780a833c6c3674d68992c31a3e4b7bbe6653a0c71af4d4372963f773db69bd7abfb9e478d30ebb384a94a4d508e6180c74046f0e9c5668f7b0207749a5e23ed9a1ea291a3607e197adb9ac369900b58c77f8aff2f916222bb4cd2fea1792a79a1b26e10313d59053d5159a66cd0bf4c001cf52ed8fede17a219205116ce333540ebccbcf32578dd51ca713653e9fc310f91127f11cbc077e6134599f
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'seameval-train:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4733140%2F8030342%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240410%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240410T162945Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dca9f73141ef4b46851e7d5e5ff63cfa86f287083b5d769f6465445454a99860dcca3be8e015a6f6500ce4cf465967c045186293b5969d29e465a9e7a061108036686e1078299385e6b10925686677021827aaaf86ca258c5b35b2df992dbf247a5f2edd0ca669ba736252766c780fb4bbf0f2e6e97e7898e793ab505ca1d86da302103f1c4db7aae6a7901d09f40acedee9ae7e7aee4cc97c9d47bbfb53300715a9e698fd8911d714e1fcbc8ef8bddccb8b751e54938eb3e03814bbdf787101e659a2f73e59cae4b1335bddc849aa85282e98d35284692ddf2e2f38fdf1c41677a0c11707c561109b164ad6302feb750732591406b7acf2b1b2475b5a6174598,seameval-val:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4743897%2F8045361%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240410%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240410T162945Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5c4a758b709047a927021a5f8639110edb77ca2ff42e5ecdf68021cadc93f5043fd21ac717166ab52625cd263e9efcef8ff80a7e8826de7bbab0e1a035bbadfddc024f4e5dc0466bfa3efa41fb7d49adfefc71e75ecfcf378268ba427a86c205dcb793ccc18c32b9b3cb398583ebd7dacfc9ac121af2a270ccc17ee139ac2ffbccb0893c6b0e246c5600560b6abc3697619b91bc51e13c23c5344f9bb0ccbbef77ba631839b8568b37a93029d04552d7b8b1280b79b8a86298e5a2c6bd07fa144a6b90da14dc6835a4f68509ad4057a71d4bd7e53be907f5ceff671c54a87db8d4828353aa0dafc554cba31c7f28df32344f86b2a2e8a40b1b3a11e5e7812dd6,seameval-test1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4743901%2F8045365%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240410%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240410T162945Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D394a2a5942fa14b008546e7b9db813a7017ff07c3e3cbd07ab91f0a9715c30feecbb2d5742f07fa88d897767c6bd1201ac790b3b205557ef262db2259aeba025141e75c1effc5e00326f992a8556ab6c3d7634163f6c6c882ac80a92784f075282b9f9537d1e517f97b270af5b6eb55128e3d063b56e3d2a44769d8407e653408c396c319ed0f38d1a7a4ea79d512f681d5a29ec403baff49444461eac8c574acf72807b3eff373172cdd7f3e0d1ae1d20077e07988dd481636b1ec2bab3ecab5c166a96c902c8ff57924aa7179689a3ca050cdcc0f286bc3f93464ac42f0e09723e4e3ea6f3e64ceca09b3bc550ba31900555239a1882e3f99bd74ea3a1808a'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch.nn.utils.rnn as rnn_utils
import os
import pandas as pd
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report

from nltk.stem import PorterStemmer
porter_stemmer = PorterStemmer()
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm
import numpy as np
device = "cuda" if torch.cuda.is_available() else "cpu"

def tokenize_text(text):
    return text.split()

def get_data(folder_path):
    datasets = []
    for file_name in os.listdir(folder_path):
        if file_name.endswith('.ann'):
            file_path = os.path.join(folder_path, file_name)
            with open(file_path, 'r') as file:
                dataset = file.read()
                datasets.append(dataset)

    extracted_data = []

    for idx, dataset in enumerate(datasets):
        lines = dataset.strip().split('\n')

        for line in lines:
            parts = line.split()
            ID = parts[0]

            if not ID.startswith('T') :
                continue

            entity = parts[1]

            keyphrase = ' '.join(parts[4:])
            tokens = word_tokenize(keyphrase)
            stem_lower_tokens = []
            for token in tokens:
                stem_lower_tokens.append(porter_stemmer.stem(token.lower()))

            extracted_data.append((stem_lower_tokens, entity))

    return extracted_data

train_data = get_data('/kaggle/input/seameval-train/train2')
val_data = get_data('/kaggle/input/seameval-val/dev')
test_data = get_data('/kaggle/input/seameval-test1/semeval_articles_test')
print(train_data[0])



word2idx = dict()
word2idx['pad'] = 0
word2idx['unk'] = 1

for text, label in train_data:
    for word in text:
        if word not in word2idx:
            word2idx[word] = len(word2idx)

print(len(word2idx))

class CustomDataset(Dataset):
    def __init__(self, data, word2idx):
        self.data = data
        self.word2idx = word2idx
        self.num_classes = 3


    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        words, label = self.data[idx]
        idx_list = [self.word2idx.get(word, 1) for word in words]  # Use 1 for 'unk' if word not in word2idx
        label_idx = 0 if label == 'Process' else 1 if label == 'Task' else 2
        return torch.tensor(idx_list), torch.tensor(label_idx)

def collate_fn(batch):
    # Sort batch by sequence length for pack_padded_sequence
    batch.sort(key=lambda x: len(x[0]), reverse=True)
    # Separate sequences and labels
    sequences, labels = zip(*batch)
    # Pad sequences
    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)
    return padded_sequences, torch.tensor(labels)

train_dataset = CustomDataset(train_data, word2idx)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)

val_dataset = CustomDataset(val_data, word2idx)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)

test_dataset = CustomDataset(test_data, word2idx)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)

class BiLSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(BiLSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.bilstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.bilstm(x)
        x = self.fc(x[:, -1, :])
        return x

# Define hyperparameters
input_size = len(word2idx)
hidden_size = 128
num_classes = 3
learning_rate = 0.001
num_epochs = 10

# Initialize the model, loss function, and optimizer
model = BiLSTMClassifier(input_size, hidden_size, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# # Training loop
# for epoch in range(num_epochs):
#     model.train()  # Set the model to training mode
#     total_loss = 0

#     for batch_idx, (data, labels) in enumerate(train_dataloader):
#         # Forward pass

#         outputs = model(data)
#         loss = criterion(outputs, labels)
#         total_loss += loss.item()

#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()

#         if (batch_idx + 1) % 100 == 0:
#             print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {total_loss / (batch_idx + 1):.4f}')

def train_model(model, train_loader, val_loader, optimizer, criterion , epochs):
    val_losses, train_losses = [], []
    val_accuracy = []

    min_loss = float('inf')
    best_model_param = None

    for epoch in range(epochs):
        t_loss = 0.0
        v_loss = 0.0
        correct_pred = 0
        tot_pred = 0
        model.train()

        print("Epoch", epoch + 1)

        for input_batch, output_batch in tqdm(train_dataloader, total=len(train_dataloader), desc="Training"):
            input_batch = input_batch.to(device)
            output_batch = output_batch.to(device)

            optimizer.zero_grad()

            output = model(input_batch)

            loss = criterion(output, output_batch)

            loss.backward()
            optimizer.step()

            t_loss += loss.item()

        t_loss = t_loss / len(train_dataloader)
        train_losses.append(t_loss)


        with torch.no_grad():
            model.eval()

            for input_batch, output_batch in tqdm(val_dataloader, total=len(val_dataloader), desc="Validation"):
                input_batch = input_batch.to(device)
                output_batch = output_batch.to(device)

                output = model(input_batch)

                loss = criterion(output, output_batch)

                v_loss += loss.item()

                _, predicted = torch.max(output, 1)

                correct_pred += (predicted == output_batch).sum().item()
                tot_pred += output_batch.size(0)

            v_loss = v_loss / len(val_dataloader)
            val_losses.append(v_loss)

            if v_loss < min_loss:
                best_model_param = model.state_dict()
                min_loss = v_loss

            val_accuracy.append(correct_pred / tot_pred)
            print("Val Acc", correct_pred / tot_pred)
            print("train l", t_loss)
            print("val l", v_loss)


    return train_losses, val_losses, val_accuracy, best_model_param

def evaluate_model(model, best_model_param, test_dataloader):
    # find the classification accuracy on test set
    model.load_state_dict(best_model_param)
    true_labels = []
    predicted_labels = []

    with torch.no_grad():
        model.eval()

        correct_pred = 0
        tot_pred = 0
        for input_batch, output_batch in tqdm(test_dataloader, total = len(test_dataloader), desc = "Testing"):
            output = model(input_batch.to(device))
            # print(type(output), output.shape)
            output_batch = output_batch.to(device).to(torch.float32)

            _, predicted = torch.max(output, 1)

            correct_pred += (predicted == output_batch).sum().item()
            tot_pred += output_batch.size(0)

            true_labels.extend(output_batch.cpu().numpy())
            predicted_labels.extend(predicted.cpu().numpy())

            accuracy = correct_pred / tot_pred

            classification_rep = classification_report(true_labels, predicted_labels)

    print("Accuracy is ", acc)
    print(classification_rep)

#         print("Test Accuracy", correct_pred / tot_pred)

def evaluate_model(model, best_model_param, test_dataloader):
    # find the classification accuracy on test set
    model.load_state_dict(best_model_param)

    with torch.no_grad():
        model.eval()

        correct_pred = 0
        tot_pred = 0
        for input_batch, output_batch in tqdm(test_dataloader, total = len(test_dataloader), desc = "Testing"):
            output = model(input_batch.to(device))
            # print(type(output), output.shape)
            output_batch = output_batch.to(device).to(torch.float32)

            _, predicted = torch.max(output, 1)
#             print(predicted)
            correct_pred += (predicted == output_batch).sum().item()
            tot_pred += output_batch.size(0)

    return correct_pred / tot_pred
#         print("Test Accuracy", correct_pred / tot_pred)

# train the model
train_losses, val_losses, val_accuracy, best_model_param = train_model(model, train_dataloader, val_dataloader, optimizer, criterion , 10)

evaluate_model(model, best_model_param, test_dataloader)